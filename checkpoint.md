# Checkpoint: MolSys-AI Project Status

This document summarizes the current status of the project, its objectives, the progress made, and the next steps.

## 1. Overall Goals (What we want)

The three main long-term objectives for this project are:

1.  **Model Retraining:** Retrain a language model to have a deep understanding of the MolSysSuite ecosystem tools.
2.  **Documentation Chatbot:** Integrate a chatbot into the documentation web pages (generated with Sphinx) of `molsysmt`, `molsysviewer`, etc., to answer questions about using the tools.
3.  **Autonomous CLI Agent:** Develop a command-line agent (`molsys-ai`) capable of executing autonomous workflows using the MolSysSuite tools at the user's request.

## 2. Progress Made (What we've done)

The initial project setup, the implementation of the first tool, the construction of the RAG system, and the final validation of the complete flow with a real language model have been completed.

### Phases 1-4: Setup, First Tool, and RAG System

- The project skeleton setup, the implementation of the first tool (`get_info`), and the construction and testing of the index for the RAG system were completed.

### Phase 5: Real LLM Integration and Environment Debugging

- **Purpose:** Replace the `StubBackend` of the `model_server` with a real language model.
- **Model Selection:** After analyzing ADRs and available options, the **`Qwen3-8B`** model (quantization `Q6_K`) was selected as the most suitable for the project.
- **Environment Setup for GPU:** An exhaustive effort was made to compile `llama-cpp-python` with GPU support, revealing deep incompatibilities in the Conda environment (`gcc` vs `nvcc`, missing CUDA development headers).
- **Strategic Decision (Fallback to CPU):** After multiple failed attempts, it was concluded that GPU compilation in the current environment was not feasible in an automated way. The decision was made to proceed with a **CPU-only** version of `llama-cpp-python` to unblock the project.
- **Model Loading on CPU:** The `model_server` was configured to use the `Qwen3-8B-Q6_K.gguf` model, which loads correctly into CPU RAM.

### Phase 6: Diagnosis and Final Validation of the RAG System

- **Error Diagnosis:** It was detected that `curl` requests were failing with an `Internal Server Error`. Log analysis revealed the cause was a 30-second `timeout` in the HTTP client (`agent/model_client.py`), which was insufficient for CPU inference.
- **Solution:** The `timeout` was increased to 300 seconds, solving the problem.
- **Milestone Reached:** The end-to-end `curl` test was successfully executed, receiving a coherent response generated by the `Qwen3-8B` model based on the context retrieved by the RAG system. **The complete flow of the RAG system with a real language model has been validated.**

## 3. Current Status

The RAG system is **fully functional in CPU mode**. The `model_server` loads the `Qwen3-8B` model, and the `docs_chat` backend orchestrates document retrieval from the vector database and response generation through the model.

The main bottleneck is the inference speed, which is slow as it does not use GPU acceleration.

## 4. Next Steps

With the RAG flow validated, the immediate next step is awaiting the hardware upgrade to enable GPU acceleration, which will dramatically improve inference speed.

1.  **Awaiting GPU Hardware Upgrade:** The inference node is currently being upgraded with RTX 2080 Ti GPUs. Once this upgrade is physically completed and verified, we will proceed with integrating `vLLM`.
    - **Action required:** User to confirm completion of GPU upgrade.

2.  **Integrate `vLLM` (Post-GPU Upgrade):** Upon successful GPU upgrade, the priority will shift to installing and configuring `vLLM` as the inference engine, leveraging the new Tensor Cores and `PagedAttention` for optimal performance. This will involve:
    - Verifying new GPUs via `nvidia-smi`.
    - Installing `vLLM` (requires Python 3.9+ and compatible CUDA).
    - Adapting `model_server/server.py` to use a `vLLMBackend`.
    - Benchmarking `vLLM` against `llama-cpp-python` (if `llama-cpp-python` is recompiled for GPU).

3.  **Improve RAG Quality:**
    - **Prompt Engineering:** Refine the prompt sent to the LLM to make more effective use of the documentation context.
    - **Chunking:** Evaluate and improve the document splitting strategy (`chunking`) in `rag/build_index.py` to optimize the relevance of retrieved snippets.

4.  **Expand Agent Tools:**
    - Begin implementing new real tools from the `molsysmt` ecosystem in `agent/tools/`, following the `ROADMAP.md` and ADRs.

5.  **Prepare Fine-Tuning Dataset (Roadmap v0.5):**
    - Start collecting and structuring data (code examples, question-answer pairs) for the future fine-tuning of the model.
