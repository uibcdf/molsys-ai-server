
# ADR-011: Evaluation strategy and model benchmarks

## Date
2025-12-04
## Status
Proposed

## Context
MolSys-AI may internally compare:
- different base models (Qwen, Llama, DeepSeek),
- different LoRAs,
- different versions of the agent.

We need a systematic way to measure improvements.

## Decision
Create a `benchmarks/` directory with:

### 1. Documentation comprehension benchmarks
- Questions about the APIs of MolSysMT/MolSysViewer/TopoMT.
- Expected answers.
- Metric: exact or partial match.

### 2. Workflow benchmarks
- Load a system.
- Compute simple measurements.
- Generate a pipeline.
- Metric: structured success/failure.

### 3. Coding benchmarks
- Write a valid script using MolSysMT.
- Metric: executability + absence of errors.

### 4. Interactive (manual) benchmarks
- Quality as perceived by experts.

## Options considered
- Only manual benchmarks → insufficient.
- Only automatic benchmarks → lose nuance.

## Justification
This balances objective and expert-driven measurements.

## Consequences
- It will be possible to compare:
  - Qwen2.5-7B vs Llama3-8B vs DeepSeek.
  - Specialized LoRAs.
  - New agent versions.

## Future notes
- Add response-time and memory metrics if you change backend.
