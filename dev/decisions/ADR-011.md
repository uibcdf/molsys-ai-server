
# ADR-011: Estrategia de evaluación y benchmarks de modelos

## Fecha
2025-12-04

## Estado
Propuesta

## Contexto
MolSys-AI podría competir internamente entre:
- distintos modelos base (Qwen, Llama, DeepSeek),
- distintas LoRAs,
- distintas versiones del agente.

Se necesita una forma sistemática de medir mejoras.

## Decisión
Crear un directorio `benchmarks/` con:

### 1. Benchmarks de comprensión de documentación
- Preguntas sobre APIs de MolSysMT/MolSysViewer/TopoMT
- Respuestas esperadas
- Métrica: match exacto o parcial

### 2. Benchmarks de workflows
- Cargar un sistema
- Calcular medidas simples
- Generar un pipeline
- Métrica: éxito / fallo estructurado

### 3. Benchmarks de coding
- Escribir un script válido usando MolSysMT
- Métrica: ejecutabilidad + ausencia de errores

### 4. Benchmarks interactivos (manuales)
- Calidad percibida por expertos.

## Opciones consideradas
- Solo benchmarks manuales → insuficientes.
- Solo benchmarks automáticos → pierden matices.

## Justificación
Equilibra mediciones objetivas y expertas.

## Consecuencias
- Se podrá comparar:
  - Qwen2.5-7B vs Llama3-8B vs DeepSeek
  - LoRAs especializadas
  - Nuevas versiones del agente

## Notas futuras
- Añadir métricas de tiempo de respuesta y memoria si cambias de backend.
