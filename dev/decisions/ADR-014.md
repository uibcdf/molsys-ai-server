# ADR-014: Re-evaluation of the Inference Engine and Impact of Future Hardware Upgrades

## Date
2025-12-11

## Status
Accepted

## Context
The project initially decided to use `llama-cpp-python` as the inference backend (ADR-003) due to its efficiency on the available GTX 1080 Ti GPUs. However, significant difficulties in compiling this library with GPU support in a Conda-managed environment prompted a strategic re-evaluation: Is `llama-cpp-python` still the right choice, and how would this choice be affected by a potential hardware upgrade?

This ADR formalizes the decision based on that discussion.

## Decision
The project adopts a two-stage, hardware-dependent strategy for the inference engine.

1.  **For Current Hardware (GTX 1080 Ti):** We re-affirm the choice of **`llama-cpp-python`** as the primary inference engine. Its efficiency with quantized GGUF models on non-Tensor-Core GPUs remains optimal for the current hardware. The immediate priority is to solve the GPU compilation issues by setting up a stable build environment with a **system-level NVIDIA CUDA Toolkit**, as this is considered an environment configuration problem, not a flaw in the choice of `llama-cpp-python`.

2.  **For Future Hardware (RTX 20-series or newer):** Upon upgrading the inference node's GPUs to the Turing architecture (e.g., RTX 2080 Ti) or newer, the project will pivot to using **`vLLM`** as the preferred inference engine. A formal benchmark between `vLLM` and `llama-cpp-python` will be conducted, but `vLLM` is expected to be superior.

## Options Considered

### 1. `llama-cpp-python` (Chosen for current hardware)
- **Pros:**
  - Unmatched VRAM efficiency and performance on older GPUs and CPUs via GGUF quantization.
  - Simple, direct Python library integration.
- **Cons:**
  - Less efficient handling of conversational context. It must re-process the entire prompt (RAG context + new question) on each turn.

### 2. `vLLM` (Chosen for future hardware)
- **Pros:**
  - High-performance engine designed for modern GPUs with **Tensor Cores**.
  - Implements **PagedAttention**, which dramatically speeds up conversational RAG by sharing and caching the KV cache for the documentation context (prefix sharing).
  - High throughput.
- **Cons:**
  - Benefits are less pronounced on older GPUs without Tensor Cores (like the GTX 1080 Ti).
  - Can be more complex to deploy than a simple library.

### 3. Hugging Face `transformers`
- **Pros:**
  - The standard, most common library.
- **Cons:**
  - Not a specialized inference engine. Less optimized for speed and memory efficiency compared to `llama-cpp-python` or `vLLM`.
  - Requires external libraries like `bitsandbytes` for on-the-fly quantization, which is less efficient than loading a pre-quantized GGUF file.

## Justification

This hardware-dependent decision provides a clear and optimal path for both the present and the future.

- For the **present**, it correctly identifies that our main obstacle is the software environment, not the choice of `llama-cpp-python`, which remains the best tool for the current hardware. This gives us a clear mandate to fix the environment rather than prematurely changing the architecture.

- For the **future**, it recognizes that a hardware upgrade to GPUs with Tensor Cores is a critical inflection point. At that stage, the architectural benefits of `vLLM`'s `PagedAttention` for our conversational RAG use case (prefix sharing) are too significant to ignore and would likely offer a massive performance boost.

This decision validates the modular design of `model_server`, which allows the backend to be swapped without affecting the agent's core logic.

## Consequences
- **Short-term:** The development focus remains on establishing a stable build environment to compile `llama-cpp-python` with GPU support on the existing GTX 1080 Ti nodes.
- **Long-term:** The project has a clear, documented trigger (GPU upgrade to RTX 20-series or newer) for a planned architectural evolution to `vLLM`, which will unlock significant performance improvements for the conversational RAG system.

## Future notes
- An immediate benchmark of `vLLM` vs. `llama-cpp-python` should be one of the first tasks performed after any GPU hardware upgrade.
