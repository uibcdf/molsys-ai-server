# ADR-017: Selection of the Foundational Base Model for the vLLM Stack

*   **Status:** Accepted
*   **Date:** December 12, 2025

## Context
Following the hardware upgrade to 3x NVIDIA RTX 2080 Ti GPUs and the migration to a `vLLM` inference stack with AWQ quantization, a new foundational language model is required. Our strategy has evolved from selecting a pre-specialized model to choosing the best possible "base platform" to create our own specialist through extensive RAG (Retrieval-Augmented Generation) and future fine-tuning. A mandatory prerequisite for any candidate is elite-level, out-of-the-box competence in programming languages, particularly Python and shell scripting.

## Decision
1.  **`meta-llama/Llama-3.1-8B-Instruct`** is selected as the primary foundational model for the `molsys-ai` project. Development will proceed using an AWQ-quantized version of this model.
2.  A **formal benchmarking suite** will be developed to empirically evaluate model performance on tasks specific to the `molsys-ai` ecosystem.
3.  This benchmark will be used in the future to compare `Llama-3.1` against other top contenders, including **`Qwen3-8B`**, **`Olmo-3-Think-7B`**, and a top-tier code specialist (e.g., `CodeLlama` or `DeepSeek-Coder`). The results of this benchmark may trigger a future re-evaluation of this decision.

## Options Considered

### 1. `Llama-3.1-8B-Instruct` (Chosen)
*   **Pros:**
    *   Represents the state-of-the-art in general reasoning and instruction following, making it the ideal "learning platform" for our own specialization via RAG and fine-tuning.
    *   Possesses world-class coding abilities, satisfying our non-negotiable baseline requirement.
    *   Its reliability and adherence to in-context information (from RAG) is critical for providing accurate, documentation-based answers.
*   **Cons:**
    *   Before fine-tuning, it might be marginally outperformed in pure code generation by hyper-specialized models.

### 2. `Qwen3-8B`
*   **Pros:**
    *   Historically shows a slight edge in coding and tool-use benchmarks.
    *   A very strong and balanced generalist model.
*   **Cons:**
    *   Its slight out-of-the-box advantage in coding is less relevant given our fine-tuning strategy. As a base platform for learning, `Llama-3.1` is considered superior due to its stronger general reasoning engine.

### 3. Code Specialists (`CodeLlama`, `DeepSeek-Coder`, `Olmo-3`)
*   **Pros:**
    *   Potentially the absolute best at pure code generation tasks out-of-the-box.
*   **Cons:**
    *   Their intense specialization could make them less "malleable" during fine-tuning.
    *   Their comparatively weaker general reasoning might hinder their ability to effectively use scientific documentation provided via RAG or to understand complex, multi-step user requests.

## Justification
The core of this decision rests on our strategy to **build our own specialist, not just use a pre-made one.**

1.  **Prioritizing the Foundation:** For a strategy based on RAG and fine-tuning, the most critical factor is the quality of the foundational model as a learning platform. `Llama-3.1` is arguably the pinnacle of general reasoning, instruction-following, and learning ability in its class.
2.  **Competence as a Prerequisite:** The choice of `Llama-3.1` does not compromise on coding ability. Its programming skills are elite. We are selecting a "gold-medal decathlete" who we will train for our specific events, confident that their foundational athletic ability is unmatched. This is preferable to choosing a world-class "sprinter" who may not be as good at the other required disciplines (scientific reasoning, dialogue).
3.  **Future-Proofing:** By selecting the best overall base model, we maximize the potential of our future fine-tuning efforts. We are building our specialist on the strongest possible foundation.

## Consequences
*   Immediate development will proceed using an AWQ-quantized version of `Llama-3.1-8B-Instruct`. The `model_server/config.example.yaml` will be updated to reflect this.
*   A new high-priority task is created: **develop a benchmarking suite** as outlined in ADR-011. This suite must include tests for code generation, RAG-based Q&A, and simple workflow execution relevant to the `molsys-ai` domain.
*   The continued relevance of `Llama-3.1` as the base model will be periodically reviewed against the results of these internal benchmarks.
