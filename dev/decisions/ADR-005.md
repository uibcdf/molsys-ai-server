
# ADR-005: Physical distribution of nodes (training vs production)

## Date
2025-12-04
## Status
Accepted

## Context
There are two available nodes:
- Node A: with a static IP address.
- Node B: without a static IP address.
Both have 3 GTX 1080 Ti GPUs.

We need to decide how to distribute responsibilities between the two nodes.

## Decision
- Node without a static IP → **training and experimentation** (LoRA/QLoRA, model trials).
- Node with a static IP → **inference / production** (serving models and public/stable APIs).

## Options considered

### 1. Train and infer on the same node
**Pros:**
- Conceptually simpler architecture.

**Cons:**
- Risk of interference between training and serving workloads.
- Potential to saturate production GPUs.
- Harder to guarantee service availability.

### 2. Separate nodes (chosen)
**Pros:**
- Clean separation and stability in production.
- Training without time constraints or user impact.
- Clear flow: training → push to Hugging Face Hub → pull in production.

**Cons:**
- Requires keeping two environments in sync (dependencies, code versions).

## Justification
Separating training and inference is a standard practice that reduces risk and improves stability in production.

## Consequences
- Clear workflow:
  - Node B: training and uploading models to HF.
  - Node A: downloading and serving the selected model.
- Scales well when more nodes or GPUs are added.

## Future notes
- Revisit the topology when new hardware is available or additional nodes are added.
