
# ADR-003: HTTP framework and model backend for the MVP

## Date
2025-12-04
## Status
Accepted

## Context
MolSys-AI needs a model server accessible via HTTP:
- for the CLI,
- for the agent,
- for the documentation chat.

It must work well on GTX 1080 Ti GPUs and allow future growth.

## Decision
- HTTP framework: **FastAPI**
- Model backend for the MVP: **llama.cpp / llama-cpp-python**
- Recommended future backend: **vLLM** when more modern GPUs are available.

## Options considered

### 1. FastAPI + llama.cpp (chosen)
**Pros:**
- Simple and lightweight.
- Very stable on older hardware.
- VRAM-efficient with quantization (Q4_K_M).
- Easy to configure on Ubuntu.
**Cons:**
- Lower throughput than vLLM on modern hardware.

### 2. FastAPI + vLLM
**Pros:**
- Very high performance and throughput.
- Supports advanced features (paginated KV cache, etc.).

**Cons:**
- Benefits much more from modern GPUs (Tensor Cores).
- May be less efficient on GTX 1080 Ti.

### 3. TGI (Text Generation Inference)
**Pros:**
- Enterprise-grade standard with many integrated features.

**Cons:**
- Heavier to deploy.
- Overkill for the MVP on current hardware.

## Justification
For the MVP, llama.cpp is the most compatible and straightforward backend on GTX 1080 Ti.  
FastAPI offers a clear, easily extensible API with good type support.

## Consequences
- MVP is fast to implement.
- Future migration to vLLM is straightforward without breaking internal APIs (the HTTP contract remains the same).

## Future notes
- Reevaluate the model backend when GPUs with > 20 GB VRAM and modern Tensor Cores are available.
