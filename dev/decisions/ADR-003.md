
# ADR-003: Framework HTTP y backend de modelo para el MVP

## Fecha
2025-12-04

## Estado
Aceptada

## Contexto
MolSys-AI necesita un servidor de modelo accesible vía HTTP:
- para el CLI,
- para el agente,
- para el chat de documentación.

Debe funcionar bien con GPUs GTX 1080 Ti y permitir crecimiento futuro.

## Decisión
- Framework HTTP: **FastAPI**
- Backend de modelo para el MVP: **llama.cpp / llama-cpp-python**
- Backend futuro recomendado: **vLLM** cuando haya GPUs más modernas.

## Opciones consideradas

### 1. FastAPI + llama.cpp (elegida)
**Pros:**
- Simple y ligero.
- Muy estable en hardware antiguo.
- Eficiente en VRAM con cuantización (Q4_K_M).
- Fácil de configurar en Ubuntu.
**Contras:**
- Menor throughput que vLLM en hardware moderno.

### 2. FastAPI + vLLM
**Pros:**
- Muy alto rendimiento y throughput.
- Soporta características avanzadas (paginated KV cache, etc.).

**Contras:**
- Se beneficia mucho más de GPUs modernas (Tensor Cores).
- Puede ser menos eficiente en GTX 1080 Ti.

### 3. TGI (Text Generation Inference)
**Pros:**
- Estándar empresarial con muchas funcionalidades integradas.

**Contras:**
- Más pesado de desplegar.
- Overkill para el MVP sobre hardware actual.

## Justificación
Para el MVP, llama.cpp es el backend más compatible y sencillo sobre GTX 1080 Ti.  
FastAPI ofrece una API clara y fácil de extender, con buen soporte de tipos.

## Consecuencias
- MVP rápido de implementar.
- Migración futura sencilla a vLLM sin romper las APIs internas (el contrato HTTP se mantiene).

## Notas futuras
- Reevaluar el backend de modelo cuando se disponga de GPUs > 20 GB VRAM y Tensor Cores modernos.
