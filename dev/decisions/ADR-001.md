
# ADR-001: Elección del modelo base inicial de MolSys-AI

## Fecha
2025-12-04

## Estado
Aceptada

## Contexto
MolSys-AI requiere un modelo de lenguaje base para:
- razonamiento científico,
- generación de código Python,
- tool-calling,
- integración con RAG,
- entrenamiento eficiente vía LoRA/QLoRA,

todo ello en un entorno con GPUs GTX 1080 Ti (11 GB VRAM).

## Decisión
El modelo base inicial será:

**Qwen2.5-7B-Instruct**

## Opciones consideradas

### 1. Qwen2.5-7B-Instruct (elegida)
**Pros:**
- Excelente razonamiento para su tamaño.
- Muy buen rendimiento en generación de código.
- Tool-calling superior.
- Eficiencia de VRAM en GPUs antiguas.
- Buen multilingüismo (inglés/español).
- Escala bien a modelos mayores (14B, 72B).
- Fácil de cuantizar (Q4/Q5) para 11 GB VRAM.

**Contras:**
- Comunidad algo menor que Llama 3 (aunque creciendo rápido).

### 2. Llama 3 8B Instruct
**Pros:**
- Gran soporte comunitario.
- Tool-calling muy bueno.
- Calidad general alta.

**Contras:**
- Inferencia algo más pesada que Qwen2.5 a igualdad de hardware.
- Rendimiento ligeramente inferior en generación de código.

### 3. DeepSeek Coder (7B)
**Pros:**
- Excelente para tareas de código.
- Entrenado con gran cantidad de repos técnicos.

**Contras:**
- Razonamiento menos generalista.
- Menos equilibrado para agente científico con RAG.

### 4. Gemma 2 7B
**Pros:**
- Muy eficiente.
- Alto rendimiento general.

**Contras:**
- Inferior para código y tool-calling en este contexto.

## Justificación
Qwen2.5-7B ofrece la mejor combinación equilibrada de:
- razonamiento,
- generación de código,
- multilingüe,
- tool-calling,
- eficiencia para GPUs 1080 Ti.

Es el mejor punto de partida para MolSys-AI como agente científico con workflows complejos.

## Consecuencias

**Positivas**
- Gran rendimiento sin necesidad de hardware moderno.
- Fácil entrenamiento de LoRAs personalizados.
- Posibilidad de escalar a Qwen2.5-14B o 72B cuando haya nuevas GPUs.

**Negativas**
- Ligera menor popularidad que Llama 3, sin impacto técnico relevante.

## Notas futuras / Revisión
- Revisar la elección cuando se disponga de GPUs con > 24 GB VRAM.
- Evaluar migración a Qwen2.5-14B o Llama 3-70B si el uso lo justifica.
