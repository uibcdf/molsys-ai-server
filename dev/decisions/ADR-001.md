
# ADR-001: Choice of the initial base model for MolSys-AI

> Note: This initial decision was later superseded after a hardware upgrade and
> a strategic pivot to a vLLM + AWQ stack. See ADR-017 for the current
> foundational model decision.

## Date
2025-12-04
## Status
Accepted

## Context
MolSys-AI requires a base language model for:
- scientific reasoning,
- Python code generation,
- tool-calling,
- integration with RAG,
- efficient training via LoRA/QLoRA,

all within an environment with GTX 1080 Ti GPUs (11 GB VRAM).

## Decision
The initial base model will be:

**Qwen2.5-7B-Instruct**

## Options considered

### 1. Qwen2.5-7B-Instruct (chosen)
**Pros:**
- Excellent reasoning for its size.
- Very strong performance in code generation.
- Superior tool-calling.
- Efficient VRAM usage on older GPUs.
- Good multilingual performance; the model can assist in both English and Spanish.
- Scales well to larger models (14B, 72B).
- Easy to quantize (Q4/Q5) for 11 GB VRAM.

**Cons:**
- Slightly smaller community than Llama 3 (although growing quickly).

### 2. Llama 3 8B Instruct
**Pros:**
- Great community support.
- Very good tool-calling.
- High overall quality.

**Cons:**
- Inference is somewhat heavier than Qwen2.5 on the same hardware.
- Slightly worse performance in code generation.

### 3. DeepSeek Coder (7B)
**Pros:**
- Excellent for code-related tasks.
- Trained on a large number of technical repositories.

**Cons:**
- Less general-purpose reasoning.
- Less balanced for a scientific agent with RAG.

### 4. Gemma 2 7B
**Pros:**
- Very efficient.
- High general performance.

**Cons:**
- Weaker for code and tool-calling in this context.

## Justification
Qwen2.5-7B offers the best overall balance of:
- reasoning,
- code generation,
- multilingual capabilities,
- tool-calling,
- efficiency on GTX 1080 Ti GPUs.

It is the best starting point for MolSys-AI as a scientific agent with complex workflows.

## Consequences

**Positive**
- Great performance without requiring modern hardware.
- Easy training of custom LoRAs.
- Possibility to scale to Qwen2.5-14B or 72B when new GPUs are available.

**Negative**
- Slightly less popular than Llama 3, without significant technical impact.

## Future notes / Review
- Revisit this choice when GPUs with > 24 GB VRAM become available.
- Evaluate migration to Qwen2.5-14B or Llama 3-70B if usage justifies it.
