# ADR-020: API symbol verification and semantic benchmark checks

## Date
2025-12-15

## Status
Accepted

## Context

MolSys-AI is intended to be a specialist assistant for MolSysSuite tools. In practice, even with
RAG enabled, a general LLM can still produce plausible-looking but **nonexistent APIs**
(e.g. inventing `msmt.fetch`), or mix APIs across tools.

This is especially harmful for:

- copy/paste code snippets,
- CLI/agent workflows that depend on correct function names,
- user trust in documentation-grounded answers.

We already mitigate “cross-tool mixing” with:

- corpus segmentation (per-project indices),
- hybrid retrieval (embedding + lexical boost),
- docstrings/API surface in the corpus (`api_surface/`) without importing upstream packages.

However, none of these guarantees that the final answer only contains real API symbols.

We also have a lightweight benchmark harness (`dev/benchmarks/`) that currently checks
formatting constraints (citations, contains/not-contains), but it does not reliably fail when
the answer mentions nonexistent functions/classes/modules.

## Decision

### 1) Add a symbol verification stage before returning an answer

When the request is MolSysSuite-related (RAG on/auto, or a tool hint is present), the chat
backend will:

1. extract candidate API symbols from the draft answer (e.g. dotted paths like
   `molsysmt.structure.get_rmsd`, `molsysviewer.view(...)`, and class/method patterns),
2. validate these symbols against a project-scoped symbol registry derived from the corpus,
3. if unknown symbols are found, force a rewrite with constraints:
   - remove/replace unknown symbols,
   - prefer known alternatives retrieved from `api_surface/` or docs,
   - if no grounded alternative exists, respond with `NOT_DOCUMENTED` and/or ask a
     clarifying question instead of inventing APIs.

This runs as a post-processing guardrail. It is not meant to be perfect parsing; it is meant
to catch the high-impact failure mode: “invented API names”.

### 1b) Add a symbol “re-read” pass for API correctness

Even when all symbols are valid, the answer can still misuse them (wrong module/function,
wrong argument names, wrong semantics).

When RAG is enabled, the backend will:

1. extract the (valid) API symbols mentioned in the answer,
2. retrieve `api_surface/` snippets for those symbols (project-scoped),
3. force a final rewrite pass that prioritizes those API snippets for correctness.

This is intended to improve accuracy for code-heavy questions without requiring fine-tuning.

### 2) Generate a machine-readable symbol registry during corpus refresh

During corpus refresh (`dev/sync_rag_corpus.py --build-api-surface`), we will generate a
machine-readable symbol registry per project (and optionally a merged global view), for
example:

- `server/chat_api/data/docs/_symbols.json`

The registry should be buildable without imports (AST parsing) and include at least:

- module names,
- top-level function names,
- class names,
- method names (optional but recommended).

The symbol verifier should not need to scan thousands of Markdown files at runtime.

### 3) Add semantic benchmark checks for nonexistent symbols

Extend the benchmark harness to support checks like:

- `symbols_must_exist: true` (or a list of expected symbols),
- `forbid_symbols: [...]`,
- `require_symbols: [...]` (when a question is explicitly about a known API call),

so that runs fail when an answer contains symbols not present in the registry (unless the
answer explicitly states `NOT_DOCUMENTED`).

## Options considered

1. **Rely on RAG alone**
   - Rejected: retrieval improves grounding but does not prevent invented symbols.

2. **Fine-tuning only**
   - Rejected as a primary solution: fine-tuning can help but does not fully eliminate
     hallucinations and is slower/expensive to iterate on compared to guardrails.

3. **Runtime imports + introspection**
   - Rejected: importing toolchains in the server environment risks dependency conflicts and
     violates the environment separation policy.

## Consequences

- The system becomes measurably safer for code generation: it should stop producing
  “nice-looking but fake” APIs.
- The benchmark suite becomes more predictive of real user experience.
- Corpus refresh gains an additional artifact (`_symbols.json`) that must be treated as
  generated data (ignored by git like the index).

## Follow-ups

- Registry generation implemented in `dev/sync_rag_corpus.py` (writes `server/chat_api/data/docs/_symbols.json` when `--build-api-surface` is enabled).
- Verifier + rewrite loop implemented in `server/chat_api/backend.py` (toggle: `MOLSYS_AI_CHAT_VERIFY_SYMBOLS`, registry path: `MOLSYS_AI_SYMBOLS_PATH`).
- Symbol re-read implemented in `server/chat_api/backend.py` (toggle: `MOLSYS_AI_CHAT_REREAD_SYMBOLS`).
- Benchmark runner extended with `--check-symbols` and per-question `symbols_must_exist`.
- Benchmark symbol checks treat dotted prefixes as valid (e.g. `molsysmt.structure` validates if `molsysmt.structure.get_rmsd` exists), matching backend behavior.
