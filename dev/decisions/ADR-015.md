# ADR-015: Inference Engine Strategy for Molsys-AI

**Date:** December 12, 2025
**Hardware Context:** 3x NVIDIA RTX 2080 Ti (33 GB Total VRAM | Turing Architecture)
**Objective:** Define the optimal software stack to maximize performance in text generation and computational chemistry workflows.

---

## 1. Executive Summary

With the hardware upgrade to the Turing architecture (RTX 20 series), the inference node is shifting from "compatibility mode" (CPU-bound) to "high-performance mode" (GPU-bound). 

It is recommended to adopt **vLLM** as the default primary engine, reserving **SGLang** and **ExLlamaV2** for specific scientific use cases or critical memory constraints.

---

## 2. Primary Engine (Production Standard): vLLM

This will be the engine `molsys-ai` utilizes 90% of the time.

* **Justification:** Leveraging the RTX Compute Capability 7.5, it utilizes *PagedAttention* to offer the best balance between ease of use, OpenAI API compatibility, and raw speed (tokens/second).
* **Recommended Configuration (when you need more context or larger models):** `tensor_parallel_size=3`. This setting logically "fuses" the three cards into a single 33 GB GPU buffer.
* **Single-GPU baseline:** When the server has low concurrency, running on a single GPU (`tensor_parallel_size=1`) is often simpler and sufficiently fast for the MVP.
* **Ideal Use Case:**
    * Standard models: Llama-3, Mistral, Gemma (7B, 8B, 13B, 34B sizes).
    * Interactive chatbots and coding assistants.
* **Limitation:** The model **must** fit entirely within VRAM. If a model requires 34 GB, vLLM will fail with an OOM (Out Of Memory) error.

---

## 3. Specialized Engines (Tactical Alternatives)

### A. SGLang (For Structured Outputs / JSON)
* **Trigger Condition:** Deploy if `molsys-ai` workflows require generating large volumes of structured data (e.g., extracting molecular properties into strict JSON) and vLLM becomes slow or hallucinates the structure.
* **Technical Advantage:** Uses a finite state machine optimized for Regex and JSON constraints. It is significantly faster than vLLM when processing "Grammar-Constrained generation."
* **Synergy:** Shares the same kernel backend as vLLM, ensuring high compatibility.

### B. ExLlamaV2 (For VRAM Maximization)
* **Trigger Condition:** Deploy when necessary to run a "State of the Art" large model (e.g., 70B or Mixtral 8x7B) that would typically not fit in 33 GB.
* **Technical Advantage:** Allows for granular quantization (e.g., **EXL2** format at 3.0 bits or 3.5 bits). This enables surgical adjustment of model size to occupy exactly 32.5 GB, which is impossible with fixed formats (AWQ/GPTQ) used by vLLM.
* **Trade-off:** Requires converting models to the `.exl2` format (or downloading pre-converted versions).

---

## 4. Legacy Engine: llama-cpp-python

* **Status:** **Deprecated** as the primary engine.
* **New Role:** Fallback / Experimentation Engine.
* **Justification:** Retained solely to execute massive models that exceed the 33 GB VRAM limit (e.g., Llama-3-70B in high precision or 100B+ models), utilizing system RAM (CPU Offloading). It is slow, but functional for sporadic testing.

---

## 5. Decision Matrix

| Criterion | **vLLM** (Recommended) | **SGLang** | **ExLlamaV2** | **llama-cpp** |
| :--- | :--- | :--- | :--- | :--- |
| **Speed (Tokens/s)** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ |
| **JSON/Schema Handling** | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ |
| **VRAM Efficiency** | ⭐⭐⭐ (AWQ/GPTQ) | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ (EXL2) | ⭐⭐⭐⭐⭐ (CPU Offload) |
| **Ease of Use** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |
| **Multi-GPU Support** | Excellent (Tensor //) | Excellent | Good | Good |

---

## 6. Implementation Roadmap

1.  **Phase 1 (Immediate):** Install **vLLM**. Configure the OpenAI-compatible API server targeting all 3 GPUs. Migrate `molsys-ai` scripts to point to this local endpoint.
2.  **Phase 2 (Validation):** Perform speed benchmarks comparing the new stack against the previous GTX 1080 Ti installation.
3.  **Phase 3 (Optimization):** If bottlenecks are detected specifically in chemical JSON generation, deploy an **SGLang** container as a drop-in replacement.

---

**Conclusion:**
Adopting **vLLM** maximizes the ROI of the new RTX 2080 Ti hardware. Moving to SGLang or ExLlamaV2 should be a decision based on specific issues (need for strict structure or lack of memory) encountered during daily operations.
