# ADR-021: Code-aware RAG (symbol cards), hybrid retrieval, and optional runtime introspection

## Date
2025-12-15

## Status
Accepted (Phase 1 implemented)

## Context

MolSys-AI must answer questions about MolSysSuite tools with high precision. In practice, even with
documentation RAG, the model may:

- invent plausible-but-nonexistent API names (e.g. `msmt.fetch`),
- mix APIs across tools (molsysmt vs molsysviewer),
- use real symbols but with incorrect signatures/semantics.

We already have:

- a literal docs snapshot + RAG indices (ADR-019),
- API symbol verification + symbol “re-read” guardrails (ADR-020),
- a minimal benchmark harness with semantic symbol checks (ADR-020).

However, the current corpus structure is still mostly “document-page chunks”. For code-heavy
questions, this is not ideal: we want retrieval to be **symbol-first** (“find the exact function/class”)
and only then use narrative docs for context.

There is also a recurring question about whether the **serving** environment should import MolSysSuite
packages to enable runtime introspection (`help()`, `inspect.signature`, `dir()`, `inspect.getsource`).

## Decision

### 1) Add a derived, code-aware corpus layer: “symbol cards”

We will generate an additional corpus layer from upstream repositories, where the primary unit of
retrieval is the **API symbol**, not a documentation page.

For each project (`molsysmt`, `molsysviewer`, `pyunitwizard`, `topomt`), we will generate one document
per symbol (function, class, and optionally method):

- canonical identity: `project.module:Symbol` (and/or dotted form `project.module.Symbol`)
- signature (best-effort from AST, without imports)
- docstring (full, normalized; include examples when present)
- source location (path + line range)
- “related symbols” (imports/calls, lightweight AST-based edges; optional)
- linked “recipes” from tests/examples when they mention the symbol (optional but recommended)

These are written under the snapshot tree, clearly separated from raw docs, e.g.:

- `server/chat_api/data/docs/<project>/symbol_cards/...`

The derived layer is additive: we keep the literal snapshot for provenance and for long-form guides.

### 2) Index tests/examples as “recipes” (high leverage)

We will treat tests and examples as first-class RAG sources, because they are often the most precise
and copy/paste-ready guidance.

Where possible, symbol cards will link to recipe snippets (test functions, doctests, example scripts)
that demonstrate correct usage.

### 2b) Add a notebook-aware tutorial layer (tutorial → section → cell)

MolSysSuite documentation relies heavily on Jupyter notebooks where workflows are spread across many
cells (markdown interleaved with code). A per-cell extraction loses critical context (imports, variable
definitions, and narrative steps).

Therefore, the derived corpus must include:

- **tutorial recipes**: one overview document per notebook (high-level context + outline + minimal setup),
- **section recipes**: multi-cell blocks grouped by headings, with stitched “preamble” code from earlier
  cells when needed (imports + definitions; best-effort use/def analysis including simple attribute
  chains and constant-key subscripts),
- **cell recipes**: the smallest unit (still useful for pinpointing one operation), but not sufficient alone.

### 3) Retrieval becomes hybrid + symbol-aware

At query time, retrieval should prefer:

1. symbol cards (exact/prefix lexical match on dotted names),
2. tests/recipes (lexical + embeddings),
3. narrative docs (embeddings + lightweight lexical boost).

We keep per-project indices and metadata filters to reduce cross-tool mixing.

### 4) Serving environment policy: do NOT import MolSysSuite by default

Even though runtime introspection can improve accuracy, importing MolSysSuite toolchains in the vLLM
serving environment is risky because:

- it couples the inference stack to upstream dependency graphs (including CUDA-related packages),
- it can break a previously working vLLM/torch/CUDA stack,
- it increases operational complexity and reduces reproducibility.

Therefore:

- `server/chat_api` and `server/model_server` must remain runnable without importing MolSysSuite.
- code-aware extraction uses AST/offline build steps, not runtime imports.

### 5) Optional runtime introspection as a separate “inspector” service (future)

If we need `help()`/`inspect.*` at runtime, we will implement it as an **optional** separate process
or service, running in a dedicated environment that can import MolSysSuite safely.

This keeps the serving stack stable while enabling “tool use” for lab-only deployments.

## Consequences

- Retrieval for API questions becomes more precise, improving practical correctness without fine-tuning.
- Benchmarks can evolve from format checks to “operational correctness” checks (symbols + usage).
- The inference environment remains stable and decoupled from upstream toolchains.
- If/when we implement runtime introspection, it is isolated behind a dedicated inspector boundary.

## Success criteria (informal)

- Drive invented APIs toward near-zero for questions covered by the symbol registry.
- Improve “operational” answers (snippets) by prioritizing recipes + symbol cards.
- Target a subjective answer quality of ~8.5/10 after adding stronger reranking and the first fine-tuned model.

## Follow-ups

## Implementation status (Phase 1)

Implemented in this repository:

- **Symbol cards (offline, no imports)**:
  - builder: `python dev/sync_rag_corpus.py --build-symbol-cards`
  - output: `server/chat_api/data/docs/<project>/symbol_cards/...`
  - manifest: `server/chat_api/data/docs/_symbol_cards.json`
- **Recipes (offline, no imports)**:
  - builder: `python dev/sync_rag_corpus.py --build-recipes`
  - outputs:
    - `server/chat_api/data/docs/<project>/recipes/notebooks/...` (code cells from `*.ipynb` in the snapshot)
    - `server/chat_api/data/docs/<project>/recipes/notebooks_sections/...` (section-level blocks grouping multiple code cells)
    - `server/chat_api/data/docs/<project>/recipes/notebooks_tutorials/...` (tutorial-level overview per notebook)
    - `server/chat_api/data/docs/<project>/recipes/tests/...` (AST-extracted test snippets; not copied verbatim)
    - `server/chat_api/data/docs/<project>/recipes/docstrings/...` (doctest-style and fenced-code examples from docstrings; AST; no imports)
    - `server/chat_api/data/docs/<project>/recipes/markdown_snippets/...` (fenced code blocks extracted from Markdown pages in the snapshot)
  - manifest: `server/chat_api/data/docs/_recipes.json`
- **Index metadata tags**:
  - `server/rag/build_index.py` stores `metadata["project"]` and `metadata["kind"]`
    (`docs|api_surface|symbol_card|recipe|recipe_section|tutorial_recipe|recipe_card|tutorial_card`).
- **BM25 sidecar (optional, offline-built)**:
  - set `MOLSYS_AI_RAG_BUILD_BM25=1` during indexing (or `dev/sync_rag_corpus.py --build-bm25`)
    to write `<index>.bm25.pkl` alongside the embedding index.
  - enable BM25 mixing at query time with `MOLSYS_AI_RAG_BM25_WEIGHT` (for example: `0.25`).
- **Serving-time prioritization for API questions**:
  - `server/chat_api/backend.py` detects API-heavy questions and prioritizes `symbol_cards/` + `recipes/`.
- **Optional offline recipe digestion using the local engine**:
  - tool: `dev/digest_recipes_llm.py`
  - output: `server/chat_api/data/docs/<project>/recipe_cards/...`

## Next steps (Phase 2+)

- Tune the API-question heuristic + per-kind quotas and measure impact on latency and answer quality.
- Extract docstring examples (doctests / `Examples:` blocks / fenced code) into `recipes/docstrings/` so they become
  retrievable “operational” snippets tied to a specific symbol.
- Index and prioritize `recipe_cards/` (generated digests) once we validate they improve answers without introducing noise.
  Note: indexing requires rebuilding the embedding indices after generation.
- Add benchmark checks for “evidence of symbol grounding”, e.g. require at least one `symbol_cards/` source for API questions.
- Consider stronger lexical search (BM25) and/or a learned reranker (after we have stable baselines).
- Define the “inspector” boundary for agent-mode deployments (separate environment/service) if runtime introspection is needed.
