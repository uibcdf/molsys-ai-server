# ADR-016: Evaluation and Adoption of AWQ as Standard Model Format

* **Status:** Accepted
* **Date:** December 12, 2025
* **Context:** Migration of the `molsys-ai` inference node to NVIDIA Turing architecture (RTX 2080 Ti) and the vLLM engine.

## Context and Problem Statement
Historically, the `molsys-ai` project utilized the **GGUF** format (via `llama.cpp`). This choice was driven by the need for broad compatibility with older hardware (GTX 1080 Ti / Pascal) and the flexibility to perform *CPU Offloading* when VRAM was insufficient.

With the recent hardware upgrade to **3x NVIDIA RTX 2080 Ti** and the strategic shift to **vLLM** as the primary inference engine, the previous standard has become suboptimal. A re-evaluation of model quantization formats was necessary to address the following changes:
1.  **vLLM Architecture:** Designed for high-throughput, GPU-resident workloads, contrasting with GGUF's CPU-centric design.
2.  **Hardware Capabilities:** The Turing architecture introduces **Tensor Cores**, specialized hardware units capable of accelerating mixed-precision matrix operations, which legacy formats do not fully exploit.
3.  **Performance Bottlenecks:** While vLLM offers experimental support for GGUF, on-the-fly dequantization introduces unnecessary latency, negating the performance gains of the new hardware.

## Decision
Based on the evaluation of current quantization methods, we decide to **adopt the AWQ (Activation-aware Weight Quantization) format** as the default standard for all models deployed on the production inference node.

We will **deprecate the use of GGUF** for this specific node, reserving it solely for local development environments on consumer laptops or edge cases where *CPU Offloading* is strictly required.

## Technical Rationale
The decision to transition to AWQ is justified by three key technical factors:

1.  **Hardware Acceleration (Tensor Cores):**
    AWQ generates optimized kernels specifically designed for NVIDIA Tensor Cores. In our architectural benchmarks, this results in a significant increase in throughput (tokens/second) compared to GGUF running on the same Turing hardware.

2.  **Inference Quality & Activation Awareness:**
    Unlike "blind" quantization methods (such as RTN - Round To Nearest), AWQ identifies and protects the top 1% of salient weights that are critical for activation. This results in superior model performance and lower perplexity degradation at 4-bit compression, which is crucial for scientific accuracy in `molsys-ai`.

3.  **Synergy with vLLM:**
    vLLM provides first-class, native support for AWQ. This ensures efficient *PagedAttention* memory management without conversion overheads, maximizing the utility of the 33 GB VRAM pool.

## Consequences

### Positive Impacts
* **Maximum Performance:** Eliminates CPU bottlenecks during inference.
* **Same Size, Higher Speed:** A 4-bit AWQ model occupies virtually the same disk and VRAM space as a GGUF Q4_K_M model but offers superior inference speeds.
* **Model Density:** Enables the efficient deployment of models such as Qwen-8B-Instruct or Llama-3-8B on a single 11GB card, leaving ample headroom for the KV Cache (context window).

### Negative Impacts / Risks
* **Loss of Flexibility (No CPU Offloading):** AWQ models **must** fit entirely within VRAM. If a model exceeds the combined memory (33 GB with Tensor Parallelism), it cannot be executed.
    * *Mitigation:* Use more aggressive quantization techniques (ExLlamaV2) or revert to `llama.cpp` only for these specific edge cases.
* **Availability:** While high, the availability of pre-quantized AWQ models on HuggingFace is slightly lower than GGUF.
    * *Mitigation:* Utilize "AutoAWQ" to manually quantize base models if public versions are unavailable.

## Implementation Notes
* When sourcing models from repositories (HuggingFace), priority will be given to those with the `-AWQ` suffix.
* Standard execution parameter in vLLM: `--quantization awq`.
* Available context window calculations will assume the model resides permanently in VRAM.
* Operational note: vLLM may require a system-level CUDA Toolkit (`nvcc`) for
  JIT compilation of attention kernels (FlashInfer). See `dev/RUNBOOK_VLLM.md`.

---
