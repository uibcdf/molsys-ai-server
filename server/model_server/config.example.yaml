
# Example configuration for the MolSys-AI model server (vLLM)

model:
  # Backend: "vllm" is now the standard for production.
  backend: "vllm"

  # The official model for the project, hosted on the uibcdf organization
  # (ADR-017). On HPC nodes it is recommended to download the model locally
  # (Hugging Face SSH + git-lfs) and point `local_path` to that directory.
  local_path: "/abs/path/to/models/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"

  # Configuration for the vLLM backend.
  # Use 1 GPU for the MVP baseline (low concurrency).
  tensor_parallel_size: 1
  # Specify the quantization method.
  quantization: "awq"
  # Random seed for generation. Keep as an explicit integer (`0` is a good default).
  # Note: vLLM v0.13 will no longer accept `null`/`None` here.
  seed: 0
  # Set a max model length (tokens). 8192 is a reasonable baseline on 11 GB GPUs.
  max_model_len: 8192
  # Conservative memory utilization for stability.
  gpu_memory_utilization: 0.80
  # Stability-first baseline: disable torch.compile/cudagraph.
  enforce_eager: true
