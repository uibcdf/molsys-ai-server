# MolSys-AI Chat API (RAG Orchestrator)

This directory contains the FastAPI chat API used by MolSys-AI. It is the
**public-facing** endpoint used by:

- the documentation widget embedded in Sphinx/MyST sites (`server/web_widget/`), and
- the `molsys-ai` CLI.

## Overview

- The backend exposes a single HTTP endpoint:
  - `POST /v1/chat`
    - Request body: JSON with
      - Either:
        - `messages: [{"role": "...", "content": "..."}]` – full conversation history (recommended).
        - `query: str` – single-turn question (backwards-compatible).
      - `k: int` (optional, default `5`) – number of RAG documents to retrieve.
      - Optional behavior flags:
        - `client: "widget" | "cli"` (default: `"widget"`)
        - `rag: "on" | "off" | "auto"` (default depends on client)
        - `sources: "on" | "off" | "auto"` (default depends on client)
    - Response body:
      - `{"answer": "...", "sources": [...]}` – model-generated answer plus optional sources metadata.
- On startup it:
  - builds an embedding-based RAG index from Markdown documents,
  - uses `rag.build_index` and `rag.retriever` to populate the index.

## Configuration

The backend is configured via environment variables:

- `MOLSYS_AI_DOCS_DIR`
  - Directory containing `*.md` documents used for RAG.
  - Default: `server/chat_api/data/docs` relative to this repository.
  - For deployments, prefer a system path (e.g. `/var/lib/molsys-ai/docs`) populated by `dev/sync_rag_corpus.py`.

- `MOLSYS_AI_DOCS_INDEX`
  - Path where the built index is stored (pickle file).
  - Default: `server/chat_api/data/rag_index.pkl`.
  - For deployments, prefer a system path (e.g. `/var/lib/molsys-ai/rag_index.pkl`).

- `MOLSYS_AI_ENGINE_URL`
  - Base URL of the MolSys-AI model engine server.
  - Default: `http://127.0.0.1:8001`.
  - The chat API uses this URL to call `POST /v1/engine/chat` via
    `agent.model_client.HTTPModelClient`.

- `MOLSYS_AI_ENGINE_API_KEY`
  - Optional API key used by the chat API when calling the model engine server.
  - Use this when the engine endpoint is protected with `MOLSYS_AI_ENGINE_API_KEYS`.

- `MOLSYS_AI_CORS_ORIGINS`
  - Optional comma-separated list of allowed browser origins for cross-origin
  widget use (e.g. local Sphinx served on a different port).
  - Example: `http://127.0.0.1:8080,http://localhost:8080`.

- `MOLSYS_AI_RAG_CHUNK_MAX_CHARS`, `MOLSYS_AI_RAG_CHUNK_MIN_CHARS`
  - Chunking parameters for index building (see `server/rag/build_index.py`).

- `MOLSYS_AI_RAG_MAX_CHUNKS_PER_SOURCE`
  - Limits how many chunks from a single source file can appear in the top-k results.

- `MOLSYS_AI_CHAT_API_KEYS`
  - Optional comma-separated list of accepted API keys for `POST /v1/chat`.
  - If unset/empty, `/v1/chat` is public (recommended for the docs widget).
  - If set, clients must send either:
    - `Authorization: Bearer <key>`, or
    - `X-API-Key: <key>`.

Request guards (recommended for public deployments):

- `MOLSYS_AI_CHAT_MAX_K` (default: 8)
- `MOLSYS_AI_CHAT_MAX_MESSAGES` (default: 30)
- `MOLSYS_AI_CHAT_MAX_MESSAGE_CHARS` (default: 4000)
- `MOLSYS_AI_CHAT_MAX_TOTAL_CHARS` (default: 20000)

Optional in-process rate limiting (minimal safety net):

- `MOLSYS_AI_CHAT_RATE_LIMIT_PER_MIN` (default: 0 = disabled)

Optional index rebuild:

- `MOLSYS_AI_DOCS_INDEX_REBUILD` (default: false)
  - If true, rebuilds the index on every startup even if the pickle exists.

Optional system prompt override:

- `MOLSYS_AI_CHAT_SYSTEM_PROMPT`

Optional citation enforcement (when `sources` are enabled):

- `MOLSYS_AI_CHAT_ENFORCE_CITATIONS` (default: true)
  - If true, and the backend returns a non-empty `sources` list, the backend will
    do a best-effort rewrite pass when the model forgets to include bracketed
    citations like `[1]` in the answer.

Optional API symbol verification (recommended):

- `MOLSYS_AI_CHAT_VERIFY_SYMBOLS` (default: true)
  - If true, the backend checks for invented MolSysSuite API symbols in the answer
    (best-effort) and forces a rewrite pass when unknown symbols are detected.
  - This requires a corpus refresh with `dev/sync_rag_corpus.py --build-api-surface`
    so that `_symbols.json` exists.
- `MOLSYS_AI_SYMBOLS_PATH`
  - Path to the symbol registry JSON (default: `<MOLSYS_AI_DOCS_DIR>/_symbols.json`).

Optional symbol “re-read” (recommended for API correctness):

- `MOLSYS_AI_CHAT_REREAD_SYMBOLS` (default: true)
  - If true, and the answer contains valid MolSysSuite API symbols, the backend
    retrieves relevant `api_surface/` snippets for those symbols and forces a
    final rewrite pass for accuracy.
- `MOLSYS_AI_CHAT_REREAD_MAX_SYMBOLS` (default: 2)
- `MOLSYS_AI_CHAT_REREAD_K_PER_SYMBOL` (default: 2)

Optional anchors map (deep links to published docs):

- `MOLSYS_AI_DOCS_ANCHORS`
  - Path to `anchors.json` generated by `dev/sync_rag_corpus.py --build-anchors`.
  - Default: `server/chat_api/data/anchors.json`.
  - When present, the backend adds `sources[i].url` pointing to `https://www.uibcdf.org/<tool>/...#Label`.

Optional segmented indices (recommended):

- `MOLSYS_AI_PROJECT_INDEX_DIR`
  - Directory containing one pickle per project, named:
    - `molsysmt.pkl`, `molsysviewer.pkl`, `pyunitwizard.pkl`, `topomt.pkl`
  - When present, and the user explicitly mentions a project name, the backend
    will query that project index instead of the global index to reduce
    cross-project API hallucinations.

## Production note: `api.uibcdf.org`

In production, the docs widget is expected to call a public API endpoint under
`https://api.uibcdf.org/v1/chat` with CORS allowing `https://uibcdf.org`.

If `80/443` are blocked to the GPU host, you may be tempted to use a non-standard
port for the API. Only do this if the center confirms that port is routed to this
host and is not already used by another service (see `dev/DEPLOY_API.md`).

## Startup behaviour

On FastAPI startup, the handler:

- Calls `rag.build_index.build_index(DOCS_SOURCE_DIR, DOCS_INDEX_PATH)` to:
  - read all `*.md`, `*.rst`, `*.txt`, and `*.ipynb` files under `DOCS_SOURCE_DIR`,
  - create `Document` objects with metadata (`path`, plus optional `section` and `label`),
  - embed chunks using the configured embedding backend
    (`sentence-transformers` by default, with a hashing fallback),
  - store the index at `DOCS_INDEX_PATH` and load it into memory.

Note on embeddings:

- For good retrieval quality, install `sentence-transformers` and use the default model
  (e.g. `pip install -e \".[rag]\"` or `pip install -e \".[dev]\"`).
- By default, sentence-transformers embeddings run on CPU even if CUDA is available, to avoid
  competing with vLLM for GPU memory. Override with:
  - `MOLSYS_AI_EMBEDDINGS_DEVICE=cuda` (or `cuda:0`, etc).
- Embedding throughput can be tuned with:
  - `MOLSYS_AI_EMBEDDINGS_BATCH_SIZE` (default: 64)
- For offline smoke tests, you can run without `sentence-transformers`; the code falls back
  to a lightweight hashing embedding model (set `MOLSYS_AI_EMBEDDINGS=hashing` to force it).
- If `DOCS_SOURCE_DIR` does not exist or contains no `*.md`, the server still starts, but retrieval returns no snippets.

When `POST /v1/chat` is called:

1. For `client="cli"` with `rag="auto"` and/or `sources="auto"`, the backend runs a small router call to `/v1/engine/chat`
   to decide whether to use RAG and whether to show sources.
2. If RAG is enabled, the backend retrieves up to `k` documents using `rag.retriever.retrieve`.
3. It builds a context block with excerpts (and optionally numbered sources).
4. It constructs a prompt with:
   - a `system` message describing the role of the assistant,
   - a `user` message containing documentation excerpts (when RAG is enabled) and the question.
5. It calls the model engine server (`/v1/engine/chat`) via `HTTPModelClient`.
6. It returns:
   - `answer`: the assistant reply,
   - `sources`: aligned with bracket indices `[1]`, `[2]`, ... when `sources` are enabled.

If the model server is not reachable or fails, a 500 error is returned.

## Running the chat API backend

In a development environment:

```bash
uvicorn chat_api.backend:app --reload
```

By default this will listen on `http://127.0.0.1:8000` and use:

- `server/chat_api/data/docs` as the source for `*.md` files (if the directory exists),
- `http://127.0.0.1:8001` as the model server URL (run `model_server` on `8001`).

## Corpus sync (recommended for real docs)

To populate `server/chat_api/data/docs/` from the live sibling repos and build the index:

```bash
python dev/sync_rag_corpus.py --clean --build-index --build-anchors
```

Notes:

- Large Markdown pages are included by default (they are truncated in the snapshot instead of skipped).
- Notebooks are compacted (outputs stripped) and bounded by `--max-bytes-ipynb`.
- If `--build-api-surface` is enabled, a symbol registry is also written to `server/chat_api/data/docs/_symbols.json`.
- `dev/sync_rag_corpus.py` always writes a coverage report to `server/chat_api/data/docs/_coverage.json`.
- To quantify coverage after a refresh, run:
  - `python dev/audit_rag_corpus.py --rescan-sources`

On multi-GPU hosts, you can speed up the index build by sharding across GPUs:

```bash
python dev/sync_rag_corpus.py --clean --build-index --build-index-parallel --index-devices 0,1,2 --build-anchors
```

If your environment has restricted network access, and the embedding model is already cached,
set offline flags to avoid long retry loops:

```bash
HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python dev/sync_rag_corpus.py --clean --build-index --build-anchors
```

Behind a reverse proxy, start uvicorn with proxy headers enabled (recommended):

```bash
uvicorn chat_api.backend:app --host 127.0.0.1 --port 8000 --proxy-headers --forwarded-allow-ips=127.0.0.1
```

## Health endpoint

- `GET /healthz` returns a small JSON payload indicating that the service is up and the index is loaded.

## In-process end-to-end smoke (no uvicorn)

If you cannot (or do not want to) run servers during a quick validation, you can run the full
pipeline in-process (retrieve → prompt → configured model backend) with:

```bash
MOLSYS_AI_EMBEDDINGS=hashing PYTHONPATH=server:client python dev/smoke_chat_inprocess.py
```

## End-to-end smoke (recommended)

Start the model server (vLLM) on `8001` (see `dev/RUNBOOK_VLLM.md`), then run:

```bash
mkdir -p /tmp/molsys_ai_docs_smoke
cat >/tmp/molsys_ai_docs_smoke/example.md <<'MD'
# Example

The example PDB id is **1VII**.
MD

MOLSYS_AI_ENGINE_URL=http://127.0.0.1:8001 \
MOLSYS_AI_DOCS_DIR=/tmp/molsys_ai_docs_smoke \
MOLSYS_AI_DOCS_INDEX=/tmp/molsys_ai_docs_smoke.pkl \
MOLSYS_AI_EMBEDDINGS=sentence-transformers \
uvicorn chat_api.backend:app --host 127.0.0.1 --port 8000
```

Then:

```bash
curl -sS -X POST http://127.0.0.1:8000/v1/chat \
  -H 'Content-Type: application/json' \
  -d '{"query":"What is the example PDB id? Reply with just the id.","k":3}'
```

## Integration with the web widget

- The Sphinx docs pilot (`docs/`) includes:
  - `docs/_static/molsys_ai_config.js`
  - `server/web_widget/molsys_ai_widget.js`

- Configuration in `molsys_ai_config.js`:

  ```js
  window.molsysAiChatConfig = window.molsysAiChatConfig || {
    mode: "placeholder", // or "backend"
    backendUrl: window.location.origin.replace(/\/+$/, "") + "/v1/chat",
    apiKey: "", // optional; sent as Authorization: Bearer <apiKey>
  };
  ```

- When `mode: "backend"` is set, the widget:
  - sends user messages to `/v1/chat`,
  - keeps conversation history in the browser and sends it as `messages`,
  - displays the `answer` field returned by this backend.

## Future work

- Replace the current pickle-based index with a FAISS-based vector store.
- Add authentication / rate limiting if needed for public deployments.
- Improve prompt construction for better use of documentation context.
