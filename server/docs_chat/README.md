# MolSys-AI Docs Chat Backend

This directory contains the FastAPI backend used by the MolSys-AI documentation
chatbot. It is intended to power the small chat widget embedded in Sphinx /
MyST documentation sites (see `docs/` and `server/web_widget/`).

## Overview

- The backend exposes a single HTTP endpoint:
  - `POST /v1/docs-chat`
    - Request body: JSON with
      - Either:
        - `messages: [{"role": "...", "content": "..."}]` – full conversation history (recommended).
        - `query: str` – single-turn question (backwards-compatible).
      - `k: int` (optional, default `5`) – number of RAG documents to retrieve.
      - Optional behavior flags:
        - `client: "widget" | "cli"` (default: `"widget"`)
        - `rag: "on" | "off" | "auto"` (default depends on client)
        - `sources: "on" | "off" | "auto"` (default depends on client)
    - Response body:
      - `{"answer": "...", "sources": [...]}` – model-generated answer plus optional sources metadata.
- On startup it:
  - builds an embedding-based RAG index from Markdown documents,
  - uses `rag.build_index` and `rag.retriever` to populate the index.

## Configuration

The backend is configured via environment variables:

- `MOLSYS_AI_DOCS_DIR`
  - Directory containing `*.md` documents used for RAG.
  - Default: `server/docs_chat/data/docs` relative to this repository.
  - For deployments, prefer a system path (e.g. `/var/lib/molsys-ai/docs`) populated by `dev/sync_rag_corpus.py`.

- `MOLSYS_AI_DOCS_INDEX`
  - Path where the built index is stored (pickle file).
  - Default: `server/docs_chat/data/rag_index.pkl`.
  - For deployments, prefer a system path (e.g. `/var/lib/molsys-ai/rag_index.pkl`).

- `MOLSYS_AI_MODEL_SERVER_URL`
  - Base URL of the MolSys-AI model server.
  - Default: `http://127.0.0.1:8001`.
  - The docs-chat backend uses this URL to call `POST /v1/chat` via
    `agent.model_client.HTTPModelClient`.

- `MOLSYS_AI_MODEL_SERVER_API_KEY`
  - Optional API key used by `docs_chat` when calling the model server.
  - Use this when `/v1/chat` is protected with `MOLSYS_AI_CHAT_API_KEYS`.

- `MOLSYS_AI_CORS_ORIGINS`
  - Optional comma-separated list of allowed browser origins for cross-origin
  widget use (e.g. local Sphinx served on a different port).
  - Example: `http://127.0.0.1:8080,http://localhost:8080`.

- `MOLSYS_AI_RAG_CHUNK_MAX_CHARS`, `MOLSYS_AI_RAG_CHUNK_MIN_CHARS`
  - Chunking parameters for index building (see `server/rag/build_index.py`).

- `MOLSYS_AI_RAG_MAX_CHUNKS_PER_SOURCE`
  - Limits how many chunks from a single source file can appear in the top-k results.

- `MOLSYS_AI_DOCS_CHAT_API_KEYS`
  - Optional comma-separated list of accepted API keys for `POST /v1/docs-chat`.
  - If unset/empty, `/v1/docs-chat` is public (recommended for the docs widget).
  - If set, clients must send either:
    - `Authorization: Bearer <key>`, or
    - `X-API-Key: <key>`.

Request guards (recommended for public deployments):

- `MOLSYS_AI_DOCS_CHAT_MAX_K` (default: 8)
- `MOLSYS_AI_DOCS_CHAT_MAX_MESSAGES` (default: 30)
- `MOLSYS_AI_DOCS_CHAT_MAX_MESSAGE_CHARS` (default: 4000)
- `MOLSYS_AI_DOCS_CHAT_MAX_TOTAL_CHARS` (default: 20000)

Optional in-process rate limiting (minimal safety net):

- `MOLSYS_AI_DOCS_CHAT_RATE_LIMIT_PER_MIN` (default: 0 = disabled)

Optional index rebuild:

- `MOLSYS_AI_DOCS_INDEX_REBUILD` (default: false)
  - If true, rebuilds the index on every startup even if the pickle exists.

Optional system prompt override:

- `MOLSYS_AI_DOCS_CHAT_SYSTEM_PROMPT`

Optional anchors map (deep links to published docs):

- `MOLSYS_AI_DOCS_ANCHORS`
  - Path to `anchors.json` generated by `dev/sync_rag_corpus.py --build-anchors`.
  - Default: `server/docs_chat/data/anchors.json`.
  - When present, the backend adds `sources[i].url` pointing to `https://www.uibcdf.org/<tool>/...#Label`.

## Production note: `api.uibcdf.org`

In production, the docs widget is expected to call a public API endpoint under
`https://api.uibcdf.org/v1/docs-chat` with CORS allowing `https://uibcdf.org`.

If `80/443` are blocked to the GPU host, you may be tempted to use a non-standard
port for the API. Only do this if the center confirms that port is routed to this
host and is not already used by another service (see `dev/DEPLOY_API.md`).

## Startup behaviour

On FastAPI startup, the handler:

- Calls `rag.build_index.build_index(DOCS_SOURCE_DIR, DOCS_INDEX_PATH)` to:
  - read all `*.md`, `*.rst`, `*.txt`, and `*.ipynb` files under `DOCS_SOURCE_DIR`,
  - create `Document` objects with metadata (`path`, plus optional `section` and `label`),
  - embed chunks using the configured embedding backend
    (`sentence-transformers` by default, with a hashing fallback),
  - store the index at `DOCS_INDEX_PATH` and load it into memory.

Note on embeddings:

- For good retrieval quality, install `sentence-transformers` and use the default model
  (e.g. `pip install -e \".[rag]\"` or `pip install -e \".[dev]\"`).
- For offline smoke tests, you can run without `sentence-transformers`; the code falls back
  to a lightweight hashing embedding model (set `MOLSYS_AI_EMBEDDINGS=hashing` to force it).
- If `DOCS_SOURCE_DIR` does not exist or contains no `*.md`, the server still starts, but retrieval returns no snippets.

When `POST /v1/docs-chat` is called:

1. For `client="cli"` with `rag="auto"` and/or `sources="auto"`, the backend runs a small router call to `/v1/chat`
   to decide whether to use RAG and whether to show sources.
2. If RAG is enabled, the backend retrieves up to `k` documents using `rag.retriever.retrieve`.
3. It builds a context block with excerpts (and optionally numbered sources).
4. It constructs a prompt with:
   - a `system` message describing the role of the assistant,
   - a `user` message containing documentation excerpts (when RAG is enabled) and the question.
5. It calls the model server (`/v1/chat`) via `HTTPModelClient`.
6. It returns:
   - `answer`: the assistant reply,
   - `sources`: aligned with bracket indices `[1]`, `[2]`, ... when `sources` are enabled.

If the model server is not reachable or fails, a 500 error is returned.

## Running the docs-chat backend

In a development environment:

```bash
uvicorn docs_chat.backend:app --reload
```

By default this will listen on `http://127.0.0.1:8000` and use:

- `server/docs_chat/data/docs` as the source for `*.md` files (if the directory exists),
- `http://127.0.0.1:8001` as the model server URL (run `model_server` on `8001`).

## Corpus sync (recommended for real docs)

To populate `server/docs_chat/data/docs/` from the live sibling repos and build the index:

```bash
python dev/sync_rag_corpus.py --clean --build-index --build-anchors
```

If your environment has restricted network access, and the embedding model is already cached,
set offline flags to avoid long retry loops:

```bash
HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python dev/sync_rag_corpus.py --clean --build-index --build-anchors
```

Behind a reverse proxy, start uvicorn with proxy headers enabled (recommended):

```bash
uvicorn docs_chat.backend:app --host 127.0.0.1 --port 8000 --proxy-headers --forwarded-allow-ips=127.0.0.1
```

## Health endpoint

- `GET /healthz` returns a small JSON payload indicating that the service is up and the index is loaded.

## In-process end-to-end smoke (no uvicorn)

If you cannot (or do not want to) run servers during a quick validation, you can run the full
pipeline in-process (retrieve → prompt → configured model backend) with:

```bash
MOLSYS_AI_EMBEDDINGS=hashing PYTHONPATH=server:client python dev/smoke_docs_chat_inprocess.py
```

## End-to-end smoke (recommended)

Start the model server (vLLM) on `8001` (see `dev/RUNBOOK_VLLM.md`), then run:

```bash
mkdir -p /tmp/molsys_ai_docs_smoke
cat >/tmp/molsys_ai_docs_smoke/example.md <<'MD'
# Example

The example PDB id is **1VII**.
MD

MOLSYS_AI_MODEL_SERVER_URL=http://127.0.0.1:8001 \
MOLSYS_AI_DOCS_DIR=/tmp/molsys_ai_docs_smoke \
MOLSYS_AI_DOCS_INDEX=/tmp/molsys_ai_docs_smoke.pkl \
MOLSYS_AI_EMBEDDINGS=sentence-transformers \
uvicorn docs_chat.backend:app --host 127.0.0.1 --port 8000
```

Then:

```bash
curl -sS -X POST http://127.0.0.1:8000/v1/docs-chat \
  -H 'Content-Type: application/json' \
  -d '{"query":"What is the example PDB id? Reply with just the id.","k":3}'
```

## Integration with the web widget

- The Sphinx docs pilot (`docs/`) includes:
  - `docs/_static/molsys_ai_config.js`
  - `server/web_widget/molsys_ai_widget.js`

- Configuration in `molsys_ai_config.js`:

  ```js
  window.molsysAiChatConfig = window.molsysAiChatConfig || {
    mode: "placeholder", // or "backend"
    backendUrl: window.location.origin.replace(/\/+$/, "") + "/v1/docs-chat",
    apiKey: "", // optional; sent as Authorization: Bearer <apiKey>
  };
  ```

- When `mode: "backend"` is set, the widget:
  - sends user messages to `/v1/docs-chat`,
  - keeps conversation history in the browser and sends it as `messages`,
  - displays the `answer` field returned by this backend.

## Future work

- Replace the current pickle-based index with a FAISS-based vector store.
- Add authentication / rate limiting if needed for public deployments.
- Improve prompt construction for better use of documentation context.
